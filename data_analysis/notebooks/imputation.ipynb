{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","import os\n","\n","import pathlib\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","import sys\n","\n","import matplotlib.pylab as pl\n","\n","# set data path\n","dir_path = pathlib.Path().absolute()\n","file = './data/MOM4_data.csv'\n","data_path = os.path.join(dir_path, file)\n","print('MOM4 data located at:',data_path)\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","''' Read dataset '''\n","# configurations for plots\n","pd.set_option('display.max_columns', None)\n","pd.set_option('display.max_rows', None)\n","\n","# read data\n","df = pd.read_csv(data_path  ,index_col=False).drop(['Unnamed: 0'], axis=1)\n","df.reset_index(drop=True, inplace=True)\n","assert df.isnull().sum().sum() == 0\n","df.head()\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","# find which (PartType, Orientation, Job) is missing\n","\n","dfgroups = df.groupby(['PartType','Orient.','Job'])\n","reps = 15\n","stencils = 5*5 # solder offset\n","t = tqdm(dfgroups)\n","for idx, (name, group) in enumerate(t):    \n","    t.set_description(f'Checking: {name}')\n","    \n","    chiptype, orient, jobtype = name\n","    # we know R0402 does not have a missing sample\n","    if chiptype == 'R0402':\n","        continue\n","    \n","    # 중복되는 job에 대해 15개 안되면 프린트\n","    jobcount = dict()\n","    for index, row in group.iterrows():\n","        job = row['Job']\n","        try:\n","            jobcount[job] += 1\n","        except:\n","            jobcount[job] = 1\n","    for key, values in jobcount.items():\n","        if values < (stencils * reps):\n","            print('Missing:\\t', chiptype, f'/ {orient} deg /', jobtype, ':', 375-jobcount[key])\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","# find which CRD is missing\n","\n","'''    \n","CRD: B if R0402, C if 0603, D if R1005\n","orient 0:  376-750 for each job\n","orient 90: 1-375 for each job\n","'''\n","\n","# pull dataframe with conditions as found above\n","missing_chip = {0: 'R0603-0-Job_2_2', 1: 'R1005-0-Job_8_5', 2: 'R1005-90-Job_0_3'}\n","\n","temp1 = df.loc[(df['PartType']=='R0603') & (df['Orient.']==0) & (df['Job']=='Job_2_2')]\n","temp2 = df.loc[(df['PartType']=='R1005') & (df['Orient.']==0) & (df['Job']=='Job_8_5')]\n","temp3 = df.loc[(df['PartType']=='R1005') & (df['Orient.']==90) & (df['Job']=='Job_0_3')]\n","temps = [temp1, temp2, temp3]\n","\n","expected_crds_90deg = np.arange(1, 375, 1)\n","expected_crds_0deg = np.arange(376, 750, 1)\n","\n","t = tqdm(temps)\n","for i, temp in enumerate(t):\n","    crds_orig = temp['CRD'].values\n","    crds = [int(crd[1:]) for crd in crds_orig]\n","    orient = temp['Orient.'].values[0]\n","    chiptype = temp['PartType'].values[0]\n","    t.set_description(f'Checking: ({chiptype}, {orient}, {crds_orig[i]})')\n","    if orient == 0:\n","        for item in expected_crds_0deg:\n","            if item not in crds:\n","                if chiptype == 'R0603':\n","                    print(f'Missing CRD in {missing_chip[i]}:\\tC{item}')\n","                elif chiptype == 'R1005':\n","                    print(f'Missing CRD in {missing_chip[i]}:\\tD{item}')\n","    else:\n","        for item in expected_crds_90deg:\n","            if item not in crds:\n","                if chiptype == 'R0603':\n","                    print(f'Missing CRD in {missing_chip[i]}:\\tC{item}')\n","                elif chiptype == 'R1005':\n","                    print(f'Missing CRD in {missing_chip[i]}:\\tD{item}')\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","missing_crd = {0: 'C395', 1: 'D492', 2: 'D143'}\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","temp1 = df.loc[(df['PartType']=='R0603') & (df['Orient.']==0) & (df['Job']=='Job_2_2')]\n","temp2 = df.loc[(df['PartType']=='R1005') & (df['Orient.']==0) & (df['Job']=='Job_8_5')]\n","temp3 = df.loc[(df['PartType']=='R1005') & (df['Orient.']==90) & (df['Job']=='Job_0_3')]\n","temps = [temp1, temp2, temp3]\n","\n","[len(t) for t in temps]\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","#### visualize (x, y)\n","\n","xavg = 'SPI_L'\n","yavg = 'SPI_W'\n","\n","# temp#_df: contains SPI_X_AVG and SPI_Y_AVG\n","temp1_df = temp1[[xavg, yavg]]\n","temp2_df = temp2[[xavg, yavg]]\n","temp3_df = temp3[[xavg, yavg]]\n","\n","# visualize X, Y\n","fig = plt.figure(figsize=(12,3), facecolor='white')\n","for idx, group in enumerate([temp1, temp2, temp3]):\n","    # show average SPI X/Y\n","    x_group = group[xavg]\n","    y_group = group[yavg]\n","    ax = fig.add_subplot(1,3,idx+1)\n","    ax.scatter(x_group, y_group, label='SPI_L,W')\n","    ax.set_xlabel(f'{xavg} (\\u03BCm)')\n","    ax.set_ylabel(f'{yavg} (\\u03BCm)')\n","    ax.set_title(missing_chip[idx])\n","    ax.grid()\n","    ax.legend(loc='upper right')\n","    \n","    ''' show for each SPI_L1/W1, SPI_L2/W2\n","    x_group = group[x1]\n","    y_group = group[y1]\n","    ax = fig.add_subplot(2,3,idx+1)\n","    ax.scatter(x_group, y_group)\n","    ax.set_xlabel(x1)\n","    ax.set_ylabel(y1)\n","    ax.set_title(name)\n","    \n","    x_group = group[x2]\n","    y_group = group[y2]\n","    ax = fig.add_subplot(2,3,idx+4)\n","    ax.scatter(x_group, y_group)\n","    ax.set_xlabel(x2)\n","    ax.set_ylabel(y2)\n","    ax.set_title(name)\n","    '''\n","fig.tight_layout()\n","# plt.show()\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","# clustering\n","\n","from collections import defaultdict\n","\n","def list_duplicates(seq):\n","    tally = defaultdict(list)\n","    for i,item in enumerate(seq):\n","        tally[item].append(i)\n","    return ((key,locs) for key,locs in tally.items() if len(locs)>1)\n","\n","# 1. scikit learn - Kmeans\n","from sklearn.cluster import KMeans\n","missing_sample = dict()\n","\n","fig = plt.figure(figsize=(18,5), facecolor='white')\n","temp_dfs = [temp1_df,temp2_df,temp3_df]\n","for temp_idx, temp_df in enumerate(temp_dfs):\n","    missing_sample[temp_idx] = []\n","    kmeans = KMeans(n_clusters=25).fit(temp_df)\n","    centroids = kmeans.cluster_centers_\n","    for dup in sorted(list_duplicates(kmeans.labels_)):\n","        if len(temp_df.iloc[dup[1],:]) < 15:\n","            missing_sample[temp_idx] = (dup[0], dup[1])\n","            chip_orient_job = missing_chip[temp_idx]\n","            print(f'{chip_orient_job}: centroid {dup[0]} (Count: {len(temp_df.iloc[dup[1],:])})')#' \\n {temp_df.iloc[dup[1],:]} \\n') # lists corresponding row indices for each centroid\n","        else:\n","            continue\n","    ax = fig.add_subplot(1,3,temp_idx+1)\n","    ax.scatter(temp_df[xavg], temp_df[yavg], c=kmeans.labels_.astype(float), marker='.',s=50, alpha=0.5, label='SPI_L,W')\n","    ax.scatter(centroids[:, 0], centroids[:, 1], c='red', s=50, label='centroid')\n","    ax.set_xlabel(f'{xavg} (\\u03BCm)')\n","    ax.set_ylabel(f'{yavg} (\\u03BCm)')\n","    ax.legend(loc='upper right')\n","    ax.grid()\n","    ax.set_title(missing_chip[temp_idx])\n","fig.tight_layout()\n","# plt.show()\n","# fig.savefig('./stencils.png')\n","\n","# 2. scipy - KMeans\n","# from scipy.cluster.vq import kmeans,vq\n","# from matplotlib import cm\n","# for temp_idx, temp_df in enumerate([temp1_df,temp2_df,temp3_df]):\n","#     centroids, _ = kmeans(temp_df, k_or_guess=25, thresh=1e-7)\n","#     plt.scatter(temp_df[xavg], temp_df[yavg], cmap=plt.get_cmap('jet'), s=50, alpha=0.5) # c=kmeans.labels_.astype(float),\n","#     plt.scatter(centroids[:, 0], centroids[:, 1], c='red', s=50)\n","#     # plt.show()v\n","\n","#     idx, _ = vq(temp1_df, centroids)\n","#     # print(idx) # idx = lists the centroid numbers for each row (of SPI)\n","#     for dup in sorted(list_duplicates(idx)):\n","#         if len(temp_df.iloc[dup[1],:]) < 15:\n","#             chip_orient_job = missing_chip[temp_idx]\n","#             print(f'{chip_orient_job}: centroid {dup[0]} (Count: {len(temp_df.iloc[dup[1],:])}) \\\n","#                     \\n {temp_df.iloc[dup[1],:]} \\n') # lists corresponding row indices for each centroid\n","\n","# from scipy.spatial import distance\n","# distance.euclidean(centroids, temp1_df)\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","# check: (parttype, orient, job) sample\n","print('====Sample====')\n","print(missing_chip[0],':')\n","print(temps[0].head(1))\n","\n","print()\n","# check centroid values\n","# missing sample contains (centroid number, index in temp_dfs[i] for the centroid)\n","# temp_dfs contains dataframe corresponding to the chosen indices (for the centroids)\n","print('====Centroids====')\n","for i, (k, v) in enumerate(missing_sample.items()):\n","    print(f'{missing_chip[k]}:\\n(index: {v}), \\n(df: {temp_dfs[i].iloc[v[1],:].head(1)})')\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","# imputation: KNN Imputer(2 neighbors)\n","from sklearn.impute import KNNImputer\n","import random\n","\n","imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n","df_temp_save = df.copy()\n","# df_temp_save = df.copy().drop(columns=['SPI_VOLUME1', 'SPI_VOLUME2', 'SPI_R','PRE_R','POST_R','SPI_VOLUME_DIFF'])\n","for i in range(len(missing_sample)):\n","    # add a nan row, impute NaN, add to original by index, reset index\n","    # missing samples = dict with key = missing chip/orient/job, value = tuple (centroid, rows that correspond to that centroid containing missing row)\n","    tmp = temps[i].iloc[missing_sample[i][1],:].append(pd.Series(dtype=float), ignore_index=True)\n","    \n","    # columns to impute: float type\n","    cols_to_impute = ['X','Y',                       \n","                      'SPI_VOLUME1','SPI_VOLUME2',                       \n","                      'SPI_X1','SPI_Y1','SPI_X2','SPI_Y2',                       \n","                      'SPI_L1','SPI_W1','SPI_L2','SPI_W2',                       \n","                      'SPI_L','SPI_W', \n","                      # 'SPI_R', \\ # compute based on imputed SPI_L, SPI_W\n","                      'PRE_X','PRE_Y','PRE_A',\\\n","                      'PRE_L','PRE_W', \\\n","                      # 'PRE_R', \\ # compute based on imputed PRE_L, PRE_W\n","                      'POST_X','POST_Y','POST_A',\\\n","                      'POST_L','POST_W', \\\n","                      # 'POST_R', \\ # compute based on imputed POST_L, POST_W\n","                      'SPI_VOLUME_MEAN', 'SPI_VOLUME_DIFF', \\\n","                      'Orient.']\n","    # drop CRD and PartType for imputation\n","    tmp = tmp[cols_to_impute]\n","\n","    # perform imputation\n","    transformed = pd.DataFrame(imputer.fit_transform(tmp), columns=cols_to_impute).round(3)\n","    imputed = transformed.iloc[len(transformed)-1, :]\n","    \n","    centroid_index_start = temp_dfs[i].iloc[missing_sample[i][1],:].index[0]\n","    centroid_index_end = temp_dfs[i].iloc[missing_sample[i][1],:].index[-1]\n","    print('size changed: from',len(temp_dfs[i].iloc[missing_sample[i][1],:]),           'to', len(transformed), '/ indices corresp. to each centroid (from orig df): from',centroid_index_start,           'to', centroid_index_end)\n","\n","    job,parttype,chipl,chipw,orient = df_temp_save.iloc[centroid_index_end, :][['Job','PartType','Chip_L','Chip_W','Orient.']]\n","    for col in list(df_temp_save.columns):\n","        if col not in cols_to_impute:\n","            # append random value to these columns\n","            if col == 'Job':\n","                imputed[col] = job\n","            elif col == 'PartType':\n","                imputed[col] = parttype\n","            elif col == 'Chip_L':\n","                imputed[col] = chipl\n","            elif col == 'Chip_W':\n","                imputed[col] = chipw\n","            elif col == 'Orient.':\n","                imputed[col] = orient\n","            elif col == 'CRD':\n","                imputed[col] = missing_crd[i]\n","            elif col == 'SPI_R':\n","                imputed[col] = np.linalg.norm((imputed['SPI_L'], imputed['SPI_W']))\n","            elif col == 'PRE_R':\n","                imputed[col] = np.linalg.norm((imputed['PRE_L'], imputed['PRE_W']))\n","            elif col == 'POST_R':\n","                imputed[col] = np.linalg.norm((imputed['POST_L'], imputed['POST_W']))\n","            elif col == 'SqueegeeCd':\n","                imputed[col] = random.choice(['R1','F1'])\n","            else:\n","                print('adding NaN for:',col)\n","                imputed[col] = np.nan\n","    assert len(imputed.index) == len(list(df_temp_save.columns))\n","#     print(imputed)\n","    df_temp_save = df_temp_save.append(imputed, ignore_index=True)\n","    \n","# scan for missing (parttype, orientation, job): \n","# >> if none printed, then none missing! \n","# >> all 15 reps, 25 stencil jobs, 81 chip jobs present\n","dfgroups = df_temp_save.groupby(['PartType','Orient.','Job'])\n","reps = 3*5     # chip offset\n","stencils = 5*5 # solder offset\n","t = tqdm(dfgroups)\n","for idx, (name, group) in enumerate(t):\n","    t.set_description(f'Checking: {name}')\n","    chiptype, orient, jobtype = name\n","    # 중복되는 job에 대해 15개 안되면 프린트\n","    jobcount = dict()\n","    for index, row in group.iterrows():\n","        job = row['Job']\n","        try:\n","            jobcount[job] += 1\n","        except:\n","            jobcount[job] = 1\n","    for key, values in jobcount.items():\n","        if values < (stencils * reps):\n","            print(chiptype, f'/ {orient} deg /', jobtype, ':', jobcount[key])\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","# check imputattion result \n","\n","new_temp1 = df_temp_save.loc[(df_temp_save['PartType']=='R0603') & (df_temp_save['Orient.']==0) & (df_temp_save['Job']=='Job_2_2')]\n","new_temp2 = df_temp_save.loc[(df_temp_save['PartType']=='R1005') & (df_temp_save['Orient.']==0) & (df_temp_save['Job']=='Job_8_5')]\n","new_temp3 = df_temp_save.loc[(df_temp_save['PartType']=='R1005') & (df_temp_save['Orient.']==90) & (df_temp_save['Job']=='Job_0_3')]\n","\n","[len(temp) for temp in [new_temp1, new_temp2, new_temp3]]\n","print('newly created rows for', missing_crd.values(),':')\n","new_temp1.loc[new_temp1['CRD']==missing_crd[0]]\n","new_temp2.loc[new_temp2['CRD']==missing_crd[1]]\n","new_temp3.loc[new_temp3['CRD']==missing_crd[2]]\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","df_imputed = df_temp_save.drop(columns=['SPI_X1','SPI_Y1','SPI_X2','SPI_Y2','SPI_L1','SPI_W1','SPI_L2','SPI_W2'])\n","df_imputed.to_csv('./data/imputed_data.csv')\n","df_imputed.head()\n","\n",""]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":2}}