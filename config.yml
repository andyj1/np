# training
train:
  device: cuda
  optimizer: Adam # must be in torch.optim ['Adam', 'SGD']
  lr: 0.004
  num_epoch: 1
  maxiter: 100
  disp: False
  num_candidates: 100     # number of sampling a new pair of candidates
  display_for_every: 50   # number of lines of training state results printed
  num_samples: 100

# acquisition
acquisition:          # by default, it uses multi-start optimization
  option: 1           # acquisition function type: [1: UpperConfidenceBound, 2: ExpectedImprovement, 3: ProbabilityOfImprovement]
  q: 1                # smallest number of candidates to be considered jointly; q=1 for single-outcome
  bounds: 20          # [2xd] tensor of lower and upper bounds for each column of X
  beta: 0.5           # the smaller value, the more exploitative acquisition function becomes
  best_f: 0.2
  num_restarts: 10    # number of starting points for multi-start acquisition function
  raw_samples: 100    # number of samples for initialization

# models
anp:
  input_dim: 2
  output_dim: 1
  hidden_dim: 128
  num_context: 100

np:
  x_dim: 2
  h_dim: 400
  r_dim: 128
  z_dim: 128
  y_dim: 1
  num_context: 100
  num_iter: 200

# toy data 
toy:
  chip: R1005
  num_samples: 150

  method: constantshift # [constantshift, shift, shiftPolar, tensionSimple, tension]
  # pre L,W
  mu1: 0
  sigma1: 10
  mu2: 0
  sigma2: 10
  # pre angle
  mu_theta: 15
  sigma_theta: 5

  # self alignment: shiftPolar
  # position shift
  x_offset: 20
  x_noise: 15
  y_offset: 20
  y_noise: 15
  # angular shift
  distance: 25
  noise: 5
  
  # self alignment: tensionSimple/tension
  mu_spi1: 40
  sigma_spi1: 10
  mu_spi2: 20
  sigma_spi2: 10
  
  # self alignment: tension
  alpha: 0.0005
  beta: 0.5

  chips:
    R0402:
      length: 400
      width: 200
    R0603:
      length: 600
      width: 300
    R1005:
      length: 1000
      width: 500

# actual data
MOM4:
  parttype: R1005 # R0402, R0603, R1005
  input_var:
    - PRE_L
    - PRE_W
    # - PRE_A
    # - SPI_L
    # - SPI_W
    # - SPI_VOLUME_DIFF
  output_var:
    - POST_L
    - POST_W
